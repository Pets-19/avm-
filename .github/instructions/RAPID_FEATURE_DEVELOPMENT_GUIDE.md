# ğŸš€ Rapid Feature Development Guide

**Purpose:** Accelerate AI-assisted feature implementation from idea to production  
**Goal:** Reduce implementation time by 60-80% while improving code quality  
**Current Time:** ~30-60 minutes per feature â†’ **Target:** 10-20 minutes

---

## ğŸ“Š Current Process Analysis

### What Worked Well âœ…
1. **Clear initial requirements** - "Segment-specific models in price per sqm card"
2. **Approval before implementation** - "Please implement the quick wins"
3. **Immediate bug reporting with screenshots** - Business Bay test case
4. **Test-driven approach** - 21 tests created automatically
5. **Comprehensive documentation** - Multiple MD files created

### What Slowed Us Down âš ï¸
1. **Token budget exhaustion** - Created 300+ line documentation files
2. **Multiple Flask restarts** - Manual process monitoring
3. **Discovery of two display locations** - Incomplete initial context
4. **Database schema exploration** - Trial and error with table/column names
5. **Back-and-forth clarifications** - "Can't see rental yield" confusion

### Time Breakdown
- **Requirements & Analysis:** 5 minutes âœ…
- **Implementation:** 15 minutes âœ…
- **Testing & Bug Discovery:** 10 minutes âš ï¸
- **Bug Fix:** 5 minutes âœ…
- **Documentation:** 20 minutes âš ï¸ (token heavy)
- **Verification:** 5 minutes âš ï¸
- **Total:** ~60 minutes

---

## ğŸ¯ Optimization Strategy

### Target Time Breakdown
- **Requirements & Analysis:** 2 minutes (-60%)
- **Implementation:** 8 minutes (-47%)
- **Testing & Bug Discovery:** 3 minutes (-70%)
- **Bug Fix:** 2 minutes (-60%)
- **Documentation:** 3 minutes (-85%)
- **Verification:** 2 minutes (-60%)
- **Total:** ~20 minutes (-67% reduction)

---

## ğŸ“ Part 1: Perfect Feature Request Template

### ğŸ”¥ Use This Template for Every Feature

```markdown
## Feature Request: [Feature Name]

### 1. CONTEXT (30 seconds)
- **Location in UI:** [Exact card/section name, line number if known]
- **Current Behavior:** [What happens now]
- **Affected Files:** [If known: app.py, templates/index.html, etc.]

### 2. REQUIREMENT (30 seconds)
- **What:** [One sentence description]
- **Why:** [Business value]
- **Expected Output:** [Visual description or example]

### 3. SCOPE (30 seconds)
- **Approach:** [Minimal/Visual/Complex - or let AI recommend]
- **Testing:** [Yes/No - default: Yes]
- **Documentation:** [Minimal/Full - default: Minimal]

### 4. CONSTRAINTS (30 seconds)
- **No Breaking Changes:** [Yes/No - default: Yes]
- **Performance:** [Any specific requirements]
- **Dependencies:** [New libraries OK? Default: No]

### 5. ACCEPTANCE CRITERIA (1 minute)
- [ ] Criterion 1
- [ ] Criterion 2
- [ ] Criterion 3

### 6. GO/NO-GO
- **Proceed:** [Yes - implement now / No - analyze first]
```

### âœ¨ Example: Our Segment Feature (Optimized)

```markdown
## Feature Request: Market Segment Badge

### 1. CONTEXT
- **Location in UI:** "Price per Sq.M" card in valuation results (templates/index.html ~line 649)
- **Current Behavior:** Shows only price, no context
- **Affected Files:** app.py (backend), templates/index.html (frontend)

### 2. REQUIREMENT
- **What:** Add market segment classification badge (Budget/Mid-Tier/Premium/Luxury/Ultra-Luxury)
- **Why:** Help users understand market positioning (+60% conversion expected)
- **Expected Output:** Badge like "ğŸ’ Luxury - Top 10%" below price

### 3. SCOPE
- **Approach:** Minimal (percentile-based, no ML)
- **Testing:** Yes (unit tests required)
- **Documentation:** Minimal (inline comments only)

### 4. CONSTRAINTS
- **No Breaking Changes:** Yes (must not affect existing valuation)
- **Performance:** <50ms added to valuation time
- **Dependencies:** No new libraries

### 5. ACCEPTANCE CRITERIA
- [ ] Backend classifies price into 5 segments
- [ ] Frontend displays badge with icon and percentile
- [ ] Badge visible in valuation results section
- [ ] Test coverage >90%

### 6. GO/NO-GO
- **Proceed:** Yes - implement now (Quick Win approach)
```

**Time Saved:** 5 minutes â†’ 2 minutes (-60%)

---

## ğŸ—ï¸ Part 2: Pre-Implementation Context Package

### Create a Project Context File (One Time Setup)

Create: `.github/instructions/PROJECT_CONTEXT.md`

```markdown
# Project Context for AI-Assisted Development

## ğŸ  Application Overview
- **Type:** Flask web application (Real estate AVM)
- **Purpose:** Property valuation for Dubai market
- **ML Model:** XGBoost (RÂ²=0.897)
- **Database:** PostgreSQL (Neon)
- **Frontend:** Vanilla JavaScript + Bootstrap

## ğŸ“ Key File Locations
- **Backend Logic:** `app.py` (3101 lines)
- **Frontend:** `templates/index.html` (2800+ lines)
- **Styles:** `static/css/style.css`
- **Scripts:** `static/js/script.js`
- **Tests:** `test_*.py` files
- **Docs:** `*.md` files (root directory)

## ğŸ—„ï¸ Database Schema

### Main Tables
```python
# properties table (main data source)
- area_en: text (location)
- prop_type_en: text (property type)
- rooms_en: text (bedrooms)
- actual_area: text (sqft, needs CAST)
- trans_value: float (price in AED)
- instance_date: text (transaction date)
- project_en: text (project name)

# Other tables: amenities, rentals, dubai_pois, project_premiums
```

### Common Queries
```python
# Get property data
SELECT area_en, prop_type_en, rooms_en, actual_area, trans_value
FROM properties
WHERE CAST(actual_area AS FLOAT) > 0
  AND trans_value > 0
  AND area_en = %s

# Get rental data
SELECT * FROM rentals WHERE location = %s
```

## ğŸ¨ Frontend Structure

### Key Sections (templates/index.html)
- **Lines 1-400:** Header, navigation, input form
- **Lines 400-450:** KPI cards (top results)
- **Lines 600-800:** Valuation details (main results)
- **Lines 800-1000:** Comparables section
- **Lines 2000-2800:** JavaScript functions

### Important JavaScript Functions
- `calculateValuation()` - Main API call
- `displayValuationResults()` - Render results
- `updateMap()` - Map integration
- `displayComparables()` - Show similar properties

### CSS Classes (Reference)
- `.detail-card` - Card containers
- `.kpi-card` - KPI boxes
- `.badge-style` - For badges/pills
- `.gradient-bg` - Gradient backgrounds

## ğŸ”§ Backend Structure (app.py)

### Key Functions
```python
# Main valuation logic
calculate_valuation_from_database(location, bedrooms, property_type, size_sqft)
  â†’ Returns: dict with valuation, comparables, analytics

# Helper functions
- get_location_coordinates(location)
- get_nearby_amenities(lat, lon)
- get_rental_yield(location, bedrooms, valuation)
- calculate_project_premium(project_name)
```

### Adding New Features (Pattern)
```python
# 1. Create helper function
def calculate_new_metric(input_data):
    """Docstring with purpose."""
    # Logic here
    return result

# 2. Integrate into main valuation
result = calculate_valuation_from_database(...)
result['new_metric'] = calculate_new_metric(...)

# 3. Return in JSON response
return jsonify(result)
```

## ğŸ§ª Testing Standards

### Required Test Coverage
- Unit tests for all calculation functions
- Edge cases: None, 0, negative, extreme values
- Integration test for main valuation flow
- Target: >90% coverage

### Test File Pattern
```python
# test_feature_name.py
import pytest
from app import calculate_new_feature

def test_normal_case():
    result = calculate_new_feature(100)
    assert result > 0

def test_edge_case_zero():
    result = calculate_new_feature(0)
    assert result is None

# Run: pytest test_feature_name.py -v
```

## ğŸš€ Deployment Process

### Flask Server Management
```bash
# Stop existing server
pkill -f "python.*app.py"

# Start new server
cd /workspaces/avm-retyn
source venv/bin/activate
nohup python app.py > flask.log 2>&1 &

# Verify running
ps aux | grep app.py
curl http://127.0.0.1:5000/
```

### Frontend Changes
- Changes take effect immediately (templates)
- **IMPORTANT:** Users must hard refresh (Ctrl+Shift+R)
- Consider cache-busting for CSS/JS changes

## ğŸ¯ Common Patterns

### Adding a New Display Card
```html
<!-- In valuation details section (~line 650) -->
<div class="detail-card">
    <div class="detail-label">NEW METRIC</div>
    <div class="detail-value" id="new-metric-value">--</div>
    <div id="new-metric-badge" style="display:none;">Badge content</div>
</div>
```

### Updating Card via JavaScript
```javascript
if (valuation.new_metric) {
    document.getElementById('new-metric-value').textContent = 
        valuation.new_metric.toLocaleString();
    document.getElementById('new-metric-badge').style.display = 'block';
}
```

### Data Flow
```
User Input (form) 
  â†’ JavaScript calculateValuation()
    â†’ POST /calculate_valuation
      â†’ app.py: calculate_valuation_from_database()
        â†’ PostgreSQL queries
        â†’ ML model prediction
        â†’ Calculate metrics
      â† Return JSON
    â† Receive response
  â† displayValuationResults()
â† Update UI
```

## âš¡ Performance Considerations

### Current Benchmarks
- Database query: 50-150ms
- ML prediction: 100-200ms
- Total valuation: 200-400ms
- **Budget for new features:** <50ms each

### Optimization Tips
- Cache database queries when possible
- Use SQL aggregations (not Python loops)
- Minimize ML model calls
- Lazy-load non-critical features

## ğŸ› Common Pitfalls

### Database
- âŒ `area` column â†’ âœ… `area_en` column
- âŒ Direct CAST â†’ âœ… Check for NULL/empty first
- âŒ String bedrooms â†’ âœ… "Studio", "1 B/R", "2 B/R" format

### Frontend
- âŒ Single element ID â†’ âœ… Check for multiple locations
- âŒ Inline styles â†’ âœ… Use CSS classes when possible
- âŒ Console.log debugging â†’ âœ… Use browser DevTools

### Testing
- âŒ Only happy path â†’ âœ… Test edge cases
- âŒ Hardcoded values â†’ âœ… Use realistic data from DB
- âŒ No cleanup â†’ âœ… Reset state between tests

## ğŸ“š Quick Reference

### Environment
- Python: 3.12
- Flask: Latest
- PostgreSQL: Neon (cloud)
- Virtual env: `/workspaces/avm-retyn/venv`

### Key Dependencies
- psycopg2: Database
- xgboost: ML model
- numpy, pandas: Data processing
- flask: Web framework

### Useful Commands
```bash
# Activate venv
source venv/bin/activate

# Run tests
pytest test_*.py -v

# Check logs
tail -f flask.log

# Database query
python -c "import psycopg2; ..."
```

---

**Last Updated:** October 12, 2025  
**Update Frequency:** After major features or architectural changes
```

**Time Saved:** AI won't need to explore/discover this context

---

## ğŸ¤– Part 3: Optimized AI Interaction Prompts

### ğŸ”¥ Level 1: Quick Feature (10-20 min)

```markdown
I need to implement a new feature quickly. Here's the complete context:

**FEATURE:** [Feature name from template above]

**PASTE TEMPLATE HERE** (from Part 1)

**IMPLEMENTATION RULES:**
1. âœ… Read PROJECT_CONTEXT.md first (don't explore)
2. âœ… Create minimal documentation (inline comments only)
3. âœ… Test while implementing (don't wait for end)
4. âœ… Deploy immediately after testing
5. âŒ Don't create separate documentation files
6. âŒ Don't explain every step (just do it)
7. âŒ Don't ask for confirmation (GO approved above)

**DELIVERABLES:**
- [ ] Backend function (if needed)
- [ ] Frontend display (if needed)
- [ ] Unit tests (>90% coverage)
- [ ] Flask restarted
- [ ] 5-line summary of what changed

**START IMMEDIATELY** - I'll interrupt if needed.
```

### ğŸ”¥ Level 2: Medium Feature (20-40 min)

```markdown
I need to implement a feature with analysis first:

**FEATURE:** [Feature name]

**REQUIREMENTS:** [Paste template]

**PHASE 1: ANALYSIS (5 min max)**
1. Read PROJECT_CONTEXT.md
2. Identify affected files
3. Propose 2-3 approaches (one sentence each)
4. Recommend one approach
5. Estimate implementation time

**PHASE 2: IMPLEMENTATION (after my approval)**
- Same rules as Level 1

**WAIT FOR MY APPROVAL** after Phase 1 before implementing.
```

### ğŸ”¥ Level 3: Complex Feature (40-60 min)

```markdown
Complex feature requiring architectural decisions:

**FEATURE:** [Feature name]

**REQUIREMENTS:** [Paste template]

**PHASE 1: DEEP ANALYSIS (10 min max)**
1. Read PROJECT_CONTEXT.md
2. Analyze database schema implications
3. Consider performance impact
4. Identify potential breaking changes
5. Propose architecture with trade-offs
6. Create mini implementation plan

**PHASE 2: IMPLEMENTATION (after approval)**
- Implement in stages with checkpoints
- Test after each stage
- Report progress every 3 steps

**PHASE 3: DOCUMENTATION**
- Create separate doc file (if complex)
- Include architecture decisions
- Document testing strategy

**WAIT FOR MY APPROVAL** after Phase 1 before implementing.
```

---

## ğŸ¯ Part 4: One-Command Testing

### Create: `test_runner.sh`

```bash
#!/bin/bash
# Rapid testing script

echo "ğŸ§ª Running all tests..."

# Activate venv
source venv/bin/activate

# Run tests with coverage
pytest test_*.py -v --cov=app --cov-report=term-missing | tee test_results.txt

# Extract summary
echo ""
echo "ğŸ“Š SUMMARY:"
grep -E "(PASSED|FAILED|ERROR)" test_results.txt | tail -5

# Check coverage
coverage report --include="app.py" | grep "TOTAL"

# Exit code
if [ ${PIPESTATUS[0]} -eq 0 ]; then
    echo "âœ… ALL TESTS PASSED"
    exit 0
else
    echo "âŒ TESTS FAILED"
    exit 1
fi
```

### Usage in Prompts

```markdown
**TESTING:** Run `./test_runner.sh` after implementation
```

**Time Saved:** Manual pytest commands â†’ 1 script

---

## ğŸš€ Part 5: Automated Deployment

### Create: `deploy.sh`

```bash
#!/bin/bash
# Rapid deployment script

echo "ğŸš€ Deploying changes..."

# Stop existing server
echo "Stopping Flask..."
pkill -f "python.*app.py"
sleep 2

# Start new server
echo "Starting Flask..."
cd /workspaces/avm-retyn
source venv/bin/activate
nohup python app.py > flask.log 2>&1 &

# Wait for startup
sleep 5

# Verify
PID=$(pgrep -f "python.*app.py")
if [ -n "$PID" ]; then
    echo "âœ… Flask running (PID: $PID)"
    
    # Test endpoint
    if curl -s http://127.0.0.1:5000/ > /dev/null; then
        echo "âœ… Server responding"
        exit 0
    else
        echo "âŒ Server not responding"
        exit 1
    fi
else
    echo "âŒ Flask failed to start"
    cat flask.log | tail -20
    exit 1
fi
```

### Usage in Prompts

```markdown
**DEPLOYMENT:** Run `./deploy.sh` after testing passes
```

**Time Saved:** Manual Flask restart â†’ 1 script

---

## ğŸ“‹ Part 6: Feature Implementation Checklist

### Create: `.github/FEATURE_CHECKLIST.md`

```markdown
# Feature Implementation Checklist

Copy this for each feature:

## Pre-Implementation
- [ ] Feature request template filled
- [ ] PROJECT_CONTEXT.md reviewed
- [ ] Approach decided and approved
- [ ] Estimated time: _____ minutes

## Implementation
- [ ] Backend function created (if needed)
- [ ] Frontend display added (if needed)
- [ ] Error handling added
- [ ] Edge cases handled
- [ ] Code follows PEP 8 / style guide

## Testing
- [ ] Unit tests created
- [ ] Edge cases tested
- [ ] Test coverage >90%
- [ ] `./test_runner.sh` passes
- [ ] Manual test in browser

## Deployment
- [ ] `./deploy.sh` runs successfully
- [ ] Feature visible in UI
- [ ] No console errors
- [ ] Performance acceptable (<50ms)

## Documentation
- [ ] Inline comments added
- [ ] Function docstrings complete
- [ ] (Optional) Separate doc file created

## Verification
- [ ] Tested in multiple browsers (if UI change)
- [ ] Verified with hard refresh
- [ ] Checked for breaking changes
- [ ] Screenshots taken (if UI change)

## Time Tracking
- Estimated: _____ min
- Actual: _____ min
- Efficiency: _____ %
```

---

## ğŸ“ Part 7: Example - Perfect Prompt

### Real Example: Next Feature Implementation

```markdown
I need to implement a new feature quickly. Here's the complete context:

**FEATURE:** Property Age Analysis

**1. CONTEXT**
- **Location in UI:** New card in valuation details section (after Price per Sq.M)
- **Current Behavior:** No age/construction year info shown
- **Affected Files:** app.py (backend), templates/index.html (frontend)

**2. REQUIREMENT**
- **What:** Show property age and construction year with age category badge
- **Why:** Users want to know if property is new/old (affects maintenance costs)
- **Expected Output:** "Built in 2015 (10 years old) ğŸ—ï¸ Established" badge

**3. SCOPE**
- **Approach:** Minimal (use transaction date as proxy for age)
- **Testing:** Yes (unit tests required)
- **Documentation:** Minimal (inline comments only)

**4. CONSTRAINTS**
- **No Breaking Changes:** Yes
- **Performance:** <50ms
- **Dependencies:** No new libraries

**5. ACCEPTANCE CRITERIA**
- [ ] Backend calculates property age from transaction date
- [ ] Classifies into 4 categories: New (<5yr), Modern (5-15yr), Established (15-30yr), Legacy (30+yr)
- [ ] Frontend displays age badge with appropriate icon
- [ ] Test coverage >90%

**6. GO/NO-GO**
- **Proceed:** Yes - implement now

**IMPLEMENTATION RULES:**
1. âœ… Read PROJECT_CONTEXT.md first
2. âœ… Test while implementing
3. âœ… Deploy immediately after testing
4. âŒ Don't create separate documentation files
5. âŒ Don't explain every step (just do it)

**DELIVERABLES:**
- [ ] Backend function
- [ ] Frontend display
- [ ] Unit tests (>90% coverage)
- [ ] Run `./deploy.sh`
- [ ] 5-line summary

**START IMMEDIATELY**
```

**Expected Time:** 15 minutes (vs 60 minutes with old process)

---

## ğŸ“Š Part 8: Metrics & Continuous Improvement

### Track These Metrics

Create: `FEATURE_LOG.md`

```markdown
# Feature Implementation Log

| Date | Feature | Est Time | Actual Time | Tests | Status | Notes |
|------|---------|----------|-------------|-------|--------|-------|
| 2025-10-11 | Segment Badge | 30 min | 60 min | 20/21 | âœ… | Bug in display location |
| 2025-10-12 | [Next feature] | 15 min | __ min | __/__ | â³ | Using new process |

## Average Time Reduction
- **Before:** 60 minutes/feature
- **After:** ___ minutes/feature
- **Improvement:** ____%

## Lessons Learned
1. [Date] - [Lesson]
2. [Date] - [Lesson]
```

### Weekly Review

Every Friday, review:
1. Average implementation time
2. Test coverage trends
3. Bug rate (bugs per feature)
4. Update PROJECT_CONTEXT.md if needed

---

## ğŸ¯ Quick Start Action Plan

### âš¡ Do This Now (30 minutes one-time setup):

1. **Create Context File** (10 min)
   ```bash
   # Copy template from Part 2
   mkdir -p .github/instructions
   nano .github/instructions/PROJECT_CONTEXT.md
   # Paste and customize
   ```

2. **Create Scripts** (10 min)
   ```bash
   # Create test runner
   nano test_runner.sh
   chmod +x test_runner.sh
   
   # Create deployment script
   nano deploy.sh
   chmod +x deploy.sh
   ```

3. **Create Templates** (10 min)
   ```bash
   # Feature request template
   nano .github/FEATURE_TEMPLATE.md
   
   # Checklist
   nano .github/FEATURE_CHECKLIST.md
   
   # Metrics log
   nano FEATURE_LOG.md
   ```

### ğŸš€ Use This For Next Feature:

```markdown
I need to implement [FEATURE NAME] quickly.

[PASTE FEATURE REQUEST TEMPLATE - filled out in 2 minutes]

**RULES:** Read PROJECT_CONTEXT.md, minimal docs, test while coding, deploy immediately

**START NOW**
```

---

## ğŸ“ˆ Expected Results

### Before (Current Process)
- â±ï¸ Time per feature: 60 minutes
- ğŸ“ Documentation: 300+ lines
- ğŸ› Bug discovery: After implementation
- ğŸ”„ Iterations: 2-3 rounds
- ğŸ“Š Test coverage: 95%

### After (Optimized Process)
- â±ï¸ Time per feature: 15-20 minutes (-67%)
- ğŸ“ Documentation: Inline only
- ğŸ› Bug discovery: During implementation
- ğŸ”„ Iterations: 1 round
- ğŸ“Š Test coverage: 90%+

### ROI Calculation
- **Features per day:** 8-10 (vs 3-4 before)
- **Quality:** Maintained or improved
- **Time saved:** 40 minutes per feature
- **Weekly savings:** 6-8 hours

---

## ğŸ“ Advanced Tips

### 1. Batch Similar Features
```markdown
I need to implement 3 related features in one go:
1. Feature A
2. Feature B  
3. Feature C

[PASTE ALL 3 TEMPLATES]

**APPROACH:** Implement shared logic first, then specific features

**ESTIMATED TOTAL TIME:** 30 minutes (vs 180 minutes separately)
```

### 2. Use AI for Test Data Generation
```markdown
Generate 50 realistic test cases for [FEATURE] covering:
- Normal cases (30)
- Edge cases (15)
- Error cases (5)

Format as pytest fixtures.
```

### 3. Parallel Development
```markdown
**TASK 1:** Implement backend function
**TASK 2:** While testing backend, create frontend HTML
**TASK 3:** While deploying, write documentation

This allows overlapping work.
```

### 4. Progressive Enhancement
```markdown
**PHASE 1:** Basic feature (10 min)
**DEPLOY & TEST**
**PHASE 2:** Enhanced version (10 min)
**DEPLOY & TEST**

Better than one 20-min deployment with hidden bugs.
```

---

## ğŸ”§ Troubleshooting

### "AI is taking too long to respond"
- âŒ Long analysis paralysis
- âœ… Use "START IMMEDIATELY" in prompt
- âœ… Set explicit time limits: "(5 min max)"

### "AI creates too much documentation"
- âŒ Asking for "comprehensive" docs
- âœ… Specify "minimal inline comments only"
- âœ… Use "NO separate doc files" rule

### "AI explores too much"
- âŒ Missing PROJECT_CONTEXT.md
- âœ… Create comprehensive context file
- âœ… Say "Read PROJECT_CONTEXT.md, don't explore"

### "Features have bugs"
- âŒ Testing after complete implementation
- âœ… "Test while implementing" rule
- âœ… Create `test_runner.sh` script

### "Can't reproduce AI's work"
- âŒ No tracking of changes
- âœ… Use FEATURE_LOG.md
- âœ… Git commit after each feature

---

## ğŸ“š Additional Resources

### Recommended Reading
1. "Prompt Engineering for Developers" (OpenAI)
2. "Test-Driven Development" (Kent Beck)
3. "Clean Code" (Robert Martin)

### Tools to Consider
- **GitHub Copilot:** Real-time code suggestions
- **pytest-cov:** Automated coverage reports
- **pre-commit hooks:** Automatic code quality checks
- **CI/CD pipeline:** Automated testing on push

### Community
- Join AI-assisted development communities
- Share your FEATURE_LOG.md metrics
- Learn from others' prompt patterns

---

## ğŸ¯ Summary - Your Action Items

### ğŸ”¥ TODAY (30 min investment):
1. âœ… Create `.github/instructions/PROJECT_CONTEXT.md`
2. âœ… Create `test_runner.sh` and `deploy.sh`
3. âœ… Create `.github/FEATURE_TEMPLATE.md`
4. âœ… Create `FEATURE_LOG.md`

### ğŸ”¥ NEXT FEATURE (15 min implementation):
1. âœ… Fill feature template (2 min)
2. âœ… Use optimized prompt from Part 3
3. âœ… Let AI implement with rules
4. âœ… Track time in FEATURE_LOG.md

### ğŸ”¥ WEEKLY (5 min review):
1. âœ… Review FEATURE_LOG.md metrics
2. âœ… Update PROJECT_CONTEXT.md if needed
3. âœ… Refine prompt templates based on results

---

## ğŸ† Success Criteria

You'll know this is working when:
- âœ… Features take <20 minutes consistently
- âœ… First deployment usually works (no bugs)
- âœ… Test coverage stays >90%
- âœ… No back-and-forth clarifications needed
- âœ… You're implementing 2-3Ã— more features per day

---

**Created:** October 12, 2025  
**Based on:** Segment Badge feature implementation analysis  
**Next Review:** October 19, 2025 (after 10 features)

---

## ğŸ’¡ Final Pro Tip

**The Golden Rule of AI-Assisted Development:**

> "The more context you provide upfront, the less time AI spends exploring.  
> The clearer your requirements, the fewer iterations needed.  
> The better your prompts, the higher the quality of output."

**Time spent on setup = 10Ã— time saved on features**

Good luck with rapid feature development! ğŸš€
